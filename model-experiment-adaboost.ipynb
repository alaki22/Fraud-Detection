{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade dagshub mlflow imbalanced-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='alaki22', repo_name='Fraud-Detection', mlflow=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-27T19:03:49.540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-27T19:03:49.540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)  \npd.set_option('display.width', None)        \npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntrain_df = train_transaction.merge(train_identity, on=\"TransactionID\", how=\"left\")\ny = train_df['isFraud']\nX = train_df.drop(columns = ['isFraud'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nprint(\"Train size:\", X_train.shape)\nprint(\"Validation size:\", X_val.shape)\nprint(\"Test size:\", X_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feature Engineering","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass FraudDataPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 nan_threshold=0.95,\n                 flag_threshold=0.1,\n                 num_strategy='median', \n                 cat_strategy='most_frequent'):\n        self.nan_threshold = nan_threshold\n        self.flag_threshold = flag_threshold\n        self.num_strategy = num_strategy\n        self.cat_strategy = cat_strategy\n        self.columns_to_drop_ = []\n        self.impute_values_ = {}\n        self.flags_to_add_ = []\n\n    def fit(self, X, y=None):\n       \n        na_ratios = X.isna().mean()\n        self.columns_to_drop_ = na_ratios[na_ratios >= self.nan_threshold].index.tolist()\n        print(self.columns_to_drop_)\n        \n        \n        remaining_cols = [c for c in X.columns if c not in self.columns_to_drop_]\n        num_cols = X[remaining_cols].select_dtypes(include='number').columns\n        cat_cols = X[remaining_cols].select_dtypes(exclude='number').columns\n        \n        for col in num_cols:\n            self.impute_values_[col] = X[col].median() if self.num_strategy == 'median' else X[col].mean()\n            \n        for col in cat_cols:\n            mode = X[col].mode()\n            self.impute_values_[col] = mode[0] if not mode.empty else 'MISSING'\n            \n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        \n        cols_to_drop = [col for col in self.columns_to_drop_ if col in X.columns]\n        X = X.drop(columns=cols_to_drop)\n    \n        for col, val in self.impute_values_.items():\n            if col in X.columns:\n                X[col] = X[col].fillna(val)\n                \n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nclass CustomPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold = 3):\n        self.threshold = threshold\n        self.woe_mappings = {}\n        self.target_encodings = {}\n        self.ohe = None\n        \n    def fit(self, X, y=None):\n        df = X.copy()\n\n        cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n        num_cols = [col for col in df.columns if df[col].dtype != 'object']\n        \n        \n        n_unique = df[cat_cols].nunique()\n        na_rate = df[cat_cols].isna().mean()\n\n        self.woe_columns = list(n_unique[(n_unique > self.threshold) & (na_rate <= 0.5)].index)\n        self.one_hot_columns = list(n_unique[(n_unique <= self.threshold) & (na_rate <= 0.5)].index)\n        \n        df['__target__'] = y\n\n       \n\n        # --- WOE ENCODING ---\n        self.woe_columns_fill_na = {\n            col: X[col].mode().iloc[0] if not X[col].mode().empty else 'missing'\n            for col in self.woe_columns\n        }\n        \n        self.woe_mappings = {}\n        for col in self.woe_columns:\n            groups = df.groupby(col)['__target__'].agg(['count', 'sum'])\n            groups.columns = ['n_obs', 'n_pos']\n            groups['n_neg'] = groups['n_obs'] - groups['n_pos']\n            groups['prop_pos'] = groups['n_pos'] / groups['n_pos'].sum()\n            groups['prop_neg'] = groups['n_neg'] / groups['n_neg'].sum()\n            groups['woe'] = np.log(groups['prop_pos'] / groups['prop_neg'])\n            groups.replace([np.inf, -np.inf], 0, inplace=True)\n            groups.fillna(0, inplace=True)\n            self.woe_mappings[col] = groups['woe'].to_dict()\n\n\n        # --- ONE-HOT ENCODER ---\n        self.ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False, dtype=int)\n        self.ohe.fit(X[self.one_hot_columns])\n\n        return self\n\n    def transform(self, X):\n        X_transformed = X.copy()\n\n\n        # --- Apply WOE ---\n        for col in self.woe_columns:\n            X_transformed[f'{col}_woe'] = X_transformed[col].map(self.woe_mappings[col])\n            X_transformed.drop(columns=col, inplace=True)\n\n\n        print(\"Check Nans\")\n        n = X_transformed.isna().mean()\n\n        na_cols = list(n[n > 0].index)\n\n        print(na_cols)\n\n        for col in na_cols:\n            name, pr = col.rsplit(\"_\", 1)\n            if pr != \"woe\":\n                print(\"Error Related to Nans\")\n\n            dic = self.woe_columns_fill_na\n            mappings = self.woe_mappings\n            X_transformed[col] = X_transformed[col].fillna(mappings[name][dic[name]])\n\n\n        # --- Apply One-Hot Encoding ---\n        ohe_array = self.ohe.transform(X[self.one_hot_columns])\n        ohe_columns = self.ohe.get_feature_names_out(self.one_hot_columns)\n        df_ohe = pd.DataFrame(ohe_array, columns=ohe_columns, index=X.index)\n\n        X_transformed.drop(columns=self.one_hot_columns, inplace=True)\n        X_transformed = pd.concat([X_transformed, df_ohe], axis=1)\n\n        return X_transformed\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feature Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CorrelationFeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.8):\n        self.threshold = threshold\n        self.features_to_drop_ = []\n\n    def fit(self, X, y=None):\n        corr_matrix = pd.DataFrame(X).corr().abs()\n        high_corr_pairs = [\n            (corr_matrix.columns[i], corr_matrix.columns[j])\n            for i in range(len(corr_matrix.columns))\n            for j in range(i + 1, len(corr_matrix.columns))\n            if corr_matrix.iloc[i, j] > self.threshold\n        ]\n        \n        features_to_drop = []\n        for feat1, feat2 in high_corr_pairs:\n            if abs(X[feat1].corr(y)) < abs(X[feat2].corr(y)):\n                features_to_drop.append(feat1)\n            else:\n                features_to_drop.append(feat2)\n        \n        self.features_to_drop_ = list(set(features_to_drop))\n        return self\n\n    def transform(self, X):\n        print(self.features_to_drop_)\n        return X.drop(columns=self.features_to_drop_, errors='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score, f1_score, classification_report\n\n\npipeline = ImbPipeline([\n    ('preprocessor', FraudDataPreprocessor(\n        nan_threshold=0.95,      \n        flag_threshold=0.05,     \n        num_strategy='median',   \n        cat_strategy='most_frequent'\n    )), \n    ('custom_encoding', CustomPreprocessor(threshold=3)), \n    ('correlation_drop', CorrelationFeatureDropper(threshold=0.8)),  \n    ('smote', SMOTE(random_state=42)),  \n    ('classifier', AdaBoostClassifier(\n        n_estimators=50,  \n        learning_rate=1.0, \n        random_state=42\n    )) \n])\n\n\npipeline.fit(X_train, y_train)\n\n\ny_train_proba = pipeline.predict_proba(X_train)[:, 1]\ny_val_proba = pipeline.predict_proba(X_val)[:, 1]\ny_test_proba = pipeline.predict_proba(X_test)[:, 1]\n\n\ny_train_pred = (y_train_proba > 0.5).astype(int)\ny_val_pred = (y_val_proba > 0.5).astype(int)\ny_test_pred = (y_test_proba > 0.5).astype(int)\n\n\nroc_auc_train = roc_auc_score(y_train, y_train_proba)\nroc_auc_val = roc_auc_score(y_val, y_val_proba)\nroc_auc_test = roc_auc_score(y_test, y_test_proba)\n\nf1_train = f1_score(y_train, y_train_pred)\nf1_val = f1_score(y_val, y_val_pred)\nf1_test = f1_score(y_test, y_test_pred)\n\n\nprint(\"\\n===== Model Performance (ROC AUC and F1 Score) =====\")\nprint(f\"Train ROC AUC: {roc_auc_train:.4f}\")\nprint(f\"Validation ROC AUC: {roc_auc_val:.4f}\")\nprint(f\"Test ROC AUC: {roc_auc_test:.4f}\")\n\nprint(f\"Train F1 Score: {f1_train:.4f}\")\nprint(f\"Validation F1 Score: {f1_val:.4f}\")\nprint(f\"Test F1 Score: {f1_test:.4f}\")\n\n\nprint(\"\\n===== Test Set Classification Report =====\")\nprint(classification_report(y_test, y_test_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"===== Model Performance (ROC AUC and F1 Score) ===== Train ROC AUC: 0.8324\n\nValidation ROC AUC: 0.8358\n\nTest ROC AUC: 0.8347\n\nTrain F1 Score: 0.3069\n\nValidation F1 Score: 0.3149\n\nTest F1 Score: 0.3039","metadata":{}},{"cell_type":"code","source":"import mlflow\nimport mlflow.sklearn\nfrom sklearn.metrics import RocCurveDisplay\nimport matplotlib.pyplot as plt\n\nmlflow.set_experiment(\"adaboost\")  \nwith mlflow.start_run(run_name=\"adaboost\"):\n    mlflow.sklearn.log_model(\n        sk_model=pipeline,          \n        artifact_path=\"model\",      \n        registered_model_name=None   \n    )\n    \n\n    mlflow.log_metric(\"roc_auc_train\", roc_auc_train)\n    mlflow.log_metric(\"roc_auc_val\", roc_auc_val)\n    mlflow.log_metric(\"roc_auc_test\", roc_auc_test)\n\n\n    RocCurveDisplay.from_predictions(\n        y_test, \n        y_test_proba, \n        name=\"Ada Boost\"\n    )\n\n    \n    plt.title('ROC Curve - Ada Boost')\n    plt.grid()\n    plt.savefig('roc_curve.png')  \n    \n    \n    mlflow.log_artifact('roc_curve.png')  \n\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}